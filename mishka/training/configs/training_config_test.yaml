base_model:
  name: mistralai/Mistral-7B-v0.1
cpu:
  max_memory_gb: 16
  max_threads: 2
  prefer_cpu: true
  use_cpu: true
data:
  max_samples: null
  test_file: ./data/processed/test.jsonl
  train_file: /home/ransomeye/rebuild/mishka/training/data/processed/train_test_small.jsonl
  validation_file: ./data/processed/validation.jsonl
evaluation:
  human_evaluation: true
  metrics:
  - loss
  - perplexity
  test_set_path: ./data/test/
gpu:
  gpu_ids:
  - 0
  use_gpu: false
lora:
  alpha: 32
  dropout: 0.1
  method: qlora
  r: 16
  target_modules:
  - q_proj
  - v_proj
  - k_proj
  - o_proj
  - gate_proj
  - up_proj
  - down_proj
optimization:
  bf16: false
  dataloader_num_workers: 1
  dataloader_pin_memory: false
  fp16: false
  gradient_checkpointing: true
registry:
  enabled: true
  model_name: mishka
  model_version: 1.0.0
  registry_path: ../../ai-model-registry
  sign_model: true
training:
  eval_steps: 50
  gradient_accumulation_steps: 8
  greater_is_better: false
  learning_rate: 0.0002
  load_best_model_at_end: true
  logging_steps: 10
  max_length: 1024
  max_steps: -1
  metric_for_best_model: loss
  num_train_epochs: 1
  output_dir: ./models
  per_device_train_batch_size: 1
  save_steps: 50
  save_total_limit: 2
  temperature: 0.0
  top_p: 1.0
  warmup_steps: 50
